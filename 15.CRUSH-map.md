Thuật toán CRUSH quyết làm thế nào để lưu trữ và khôi phục data bằng cách tính toán data location

### CRUSH Location

Vị trí của OSD trong trong bản đồ phân cấp CRUSH được gọi là **crush location**. Vị trí này có dạng key và value để mô tả vị trí. Ví dụ nếu một OSD cụ thể nằm trong row, rack, chassis và host là một phần của "default" CRUSH tree, crush location sẽ được mô tả như sau:

``root=default row=a rack=a2 chassis=a2a host=a2a1``

Note:

- Thứ tự các key không quan trọng
- Key name (bên trái =) phải là một CRUSH type hợp lệ. Mặc định gồm :  root, datacenter, room, row, pod, pdu, rack, chassis và host
- Không cần xác định tất cả các key. Ví dụ, mặc định thì Ceph sẽ set **ceph-osd** location ở ``root=default host=HOSTNAME``

crush location cho OSD thường được thể hiện qua **crush location** option được set trong ``ceph.conf``. Mỗi khi OSD start, nó sẽ xác minh nó ở đúng vị trí trong CRUSH map, nếu không nó sẽ tự di chuyển chính nó. Để vô hiệu hóa việc tự động quản lý CRUSH map, thêm dòng sau vào config file trong phần [OSD]

``osd crush update on start = false``

### CRUSH STRUCTURE

CRUSH map gồm một hệ thống phân cấp mô tả topology vật lý của cluster và rules quy định policy về cách lưu trữ data trên thiết bị. 

#### Device

Device là ``ceph-osd`` daemon có thể lưu trữ data. Device được xác định bởi một id và tên thường là osd.N trong đó N là device id

Device cũng có ``device class`` được gắn liền với chúng (ví dụ: hdd hoặc ssd)

#### Types và buckets

Buckets là thuật ngữ cho các node trong bản đồ phân cấp như: hosts, racks, rows, ... CRUSH map định nghĩa các type được dùng để mô tả các node. Mặc định các type bao gồm: osd (hoặc device), host, chassis, rack, row, pdu, pod, room, datacenter, region, root,...

Mỗi node(device hoặc bucket) trong hệ thống phân cấp có chỉ số ``weight`` cho thấy tỷ lệ tương đối của tổng số dữ liệu mà thiết bị hoặc hệ thống phân cấp có thể lưu trữ

#### Rules

Rules quy định policy về cách dữ liệu được phân phối trên các devices trong hệ thống phân cấp

CRUSH rules xác định vị trí và và replication hoặc distribution policy cho phép ta xác định chính xác nơi CRUSH phân phối các bản sao.

Hầu hết ta có thể tạo CRUSH rules thông qua CLI bằng cách xác định ``pool type`` sẽ được sử dụng cho replicated hoặc erasure coded, ``failure domain`` và tùy chọn ``device class``. 

Ta có thể xem các rule được xác định cho cluster:

``ceph osd crush rule ls``

Ta có thể xem nội dung của rule:

``ceph osd crush rule dump``

#### Device classes

Mỗi device có một tùy chọn ``class`` gắn với nó. Mặc định, OSDs tự động thiết lập class của chúng khi khởi động có thể là: hdd, ssd hoặc nvme dựa trên4

### Modifying the crush map

#### Add/move OSD

Để add hoặc di chuyển OSD trong CRUSH map của một cluster đang chạy

``ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]``

Ví dụ add osd.0 vào hệ thống phân cấp hoặc di chuyển OSD

``ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1``

#### Điều chỉnh OSD Weight

Để chiều chỉnh crush weight của OSD trong CRUSH map thực thi câu lệnh:

``ceph osd crush reweight {name} {weight}``

#### Remove OSD

``ceph osd crush remove {name}``

#### Add a bucket

Để add backet vào CRUSH map thực thi câu lệnh ``ceph osd crush add-bucket``

``ceph osd crush add-bucket {bucket-name} {bucket-type}``

#### Di chuyển bucket

``ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]``

#### Xóa bucket

``ceph osd crush remove {bucket-name}``

#### Create rule cho replicated pool
